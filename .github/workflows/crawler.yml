name: Web Crawler

on:
  # Allow manual triggering
  workflow_dispatch:
  # Run on push to main branch
  push:
    branches: [ main ]
  # Schedule to run every 6 hours
  schedule:
    - cron: '0 */6 * * *'
permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  crawler:
    runs-on: ubuntu-latest
    timeout-minutes: 300  # 5 hours max runtime
    
    strategy:
      matrix:
        # Reduce to single instance to avoid resource exhaustion
        instance: [1]
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4
        
    - name: Create bot directories
      run: |
        cd crawler
        make init
        
    - name: Copy crawler source to bot directories
      run: |
        cd crawler
        make copy
        
    - name: Run crawlers (with timeout)
      timeout-minutes: 270  # 4.5 hours
      run: |
        cd crawler
        echo "Starting crawlers at $(date)"
        echo "Available bot directories:"
        ls -la bot_*/
        
        # Run a smaller subset first to test
        echo "Running first 10 bots only to prevent resource exhaustion..."
        
        # Start first 10 bots with staggered delays
        for i in {1..10}; do
          echo "Starting bot_$i at $(date)"
          cd bot_$i && python3 crawler.py https://$(sed -n "${i}p" ../urls.txt 2>/dev/null || echo "google.com") &
          cd ..
          sleep 5  # 5 second delay between starts
        done
        
        # Wait for all background processes
        echo "Waiting for all crawlers to complete..."
        wait
        
        echo "All crawlers finished at $(date)"
        
    - name: Collect crawl results
      if: always()  # Run even if crawlers timeout
      run: |
        cd crawler
        python3 join_crawlres.py
        
    - name: Upload crawl results as artifact
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: crawl-results-${{ matrix.instance }}-${{ github.run_number }}
        path: |
          crawler/crawlres.json
          crawler/bot_*/crawlres.json
        retention-days: 30
        
    - name: Display crawl statistics
      if: always()
      run: |
        echo "=== Crawl Statistics ==="
        if [ -f crawler/crawlres.json ]; then
          echo "Combined results file size: $(du -h crawler/crawlres.json | cut -f1)"
          echo "Total URLs found: $(python3 -c "import json; data=json.load(open('crawler/crawlres.json')); print(len(data) if isinstance(data, list) else len(data.get('site_known', [])))" 2>/dev/null || echo "Error reading file")"
        fi
        
        echo "Individual bot results:"
        for bot_dir in crawler/bot_*; do
          if [ -f "$bot_dir/crawlres.json" ]; then
            bot_name=$(basename "$bot_dir")
            file_size=$(du -h "$bot_dir/crawlres.json" | cut -f1)
            url_count=$(python3 -c "import json; data=json.load(open('$bot_dir/crawlres.json')); print(len(data.get('site_known', [])))" 2>/dev/null || echo "0")
            echo "$bot_name: $file_size, $url_count URLs"
          fi
        done

  # Optional: Deploy results to GitHub Pages
  deploy-results:
    runs-on: ubuntu-latest
    needs: crawler
    if: always()
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Download all crawl artifacts
      uses: actions/download-artifact@v4
      with:
        path: artifacts/
        pattern: crawl-results-*
        merge-multiple: true
        
    - name: Combine all results
      run: |
        mkdir -p public
        echo "# Web Crawler Results" > public/README.md
        echo "Generated on: $(date)" >> public/README.md
        echo "" >> public/README.md
        
        # List all artifact files
        echo "## Available Results" >> public/README.md
        find artifacts/ -name "*.json" -type f | while read file; do
          rel_path=$(echo "$file" | sed 's|artifacts/||')
          size=$(du -h "$file" | cut -f1)
          echo "- [$rel_path]($rel_path) ($size)" >> public/README.md
        done
        
        # Copy all JSON files to public directory
        find artifacts/ -name "*.json" -type f -exec cp {} public/ \;
        
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      if: github.ref == 'refs/heads/main'
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./public
        
  # Cleanup old artifacts
  cleanup:
    runs-on: ubuntu-latest
    needs: [crawler, deploy-results]
    if: always()
    
    steps:
    - name: Delete old artifacts
      uses: actions/github-script@v6
      with:
        script: |
          const artifacts = await github.rest.actions.listArtifactsForRepo({
            owner: context.repo.owner,
            repo: context.repo.repo,
            per_page: 100
          });
          
          // Keep only the 10 most recent artifacts
          const sortedArtifacts = artifacts.data.artifacts
            .filter(artifact => artifact.name.startsWith('crawl-results-'))
            .sort((a, b) => new Date(b.created_at) - new Date(a.created_at));
          
          const toDelete = sortedArtifacts.slice(10);
          
          for (const artifact of toDelete) {
            console.log(`Deleting artifact: ${artifact.name}`);
            await github.rest.actions.deleteArtifact({
              owner: context.repo.owner,
              repo: context.repo.repo,
              artifact_id: artifact.id
            });
          }
