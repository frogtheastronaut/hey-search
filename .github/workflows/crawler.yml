name: Web Crawler

on:
  # Allow manual triggering
  workflow_dispatch:
  # Run on push to main branch
  push:
    branches: [ main ]
  # Schedule to run every 6 hours
  schedule:
    - cron: '0 */6 * * *'
permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  crawler:
    runs-on: ubuntu-latest
    timeout-minutes: 25  # 25 minutes max runtime
    
    strategy:
      matrix:
        # Single instance
        instance: [1]
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4
        
    - name: Create bot directories
      run: |
        cd crawler
        make init
        
    - name: Copy crawler source to bot directories
      run: |
        cd crawler
        make copy
        
    - name: Run crawlers (with timeout)
      timeout-minutes: 20  # 20 minutes
      run: |
        cd crawler
        echo "Starting all 50 crawlers at $(date)"
        echo "Available bot directories:"
        ls -la bot_*/
        
        # Use the makefile to start all bots
        timeout 1200 make run-all || echo "Crawlers timed out after 20 minutes"
        
        echo "All crawlers finished at $(date)"
        
    - name: Collect crawl results
      if: always()  # Run even if crawlers timeout
      run: |
        cd crawler
        python3 join_crawlres.py
        
    - name: Upload crawl results as artifact
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: crawl-results-${{ matrix.instance }}-${{ github.run_number }}
        path: |
          crawler/crawlres.json
          crawler/bot_*/crawlres.json
        retention-days: 30
        
    - name: Display crawl statistics
      if: always()
      run: |
        echo "=== Crawl Statistics ==="
        if [ -f crawler/crawlres.json ]; then
          echo "Combined results file size: $(du -h crawler/crawlres.json | cut -f1)"
          echo "Total URLs found: $(python3 -c "import json; data=json.load(open('crawler/crawlres.json')); print(len(data) if isinstance(data, list) else len(data.get('site_known', [])))" 2>/dev/null || echo "Error reading file")"
        fi
        
        echo "Individual bot results:"
        for bot_dir in crawler/bot_*; do
          if [ -f "$bot_dir/crawlres.json" ]; then
            bot_name=$(basename "$bot_dir")
            file_size=$(du -h "$bot_dir/crawlres.json" | cut -f1)
            url_count=$(python3 -c "import json; data=json.load(open('$bot_dir/crawlres.json')); print(len(data.get('site_known', [])))" 2>/dev/null || echo "0")
            echo "$bot_name: $file_size, $url_count URLs"
          fi
        done

  # Cleanup old artifacts
  cleanup:
    runs-on: ubuntu-latest
    needs: crawler
    if: always()
    
    steps:
    - name: Delete old artifacts
      uses: actions/github-script@v6
      with:
        script: |
          const artifacts = await github.rest.actions.listArtifactsForRepo({
            owner: context.repo.owner,
            repo: context.repo.repo,
            per_page: 100
          });
          
          // Keep only the 10 most recent artifacts
          const sortedArtifacts = artifacts.data.artifacts
            .filter(artifact => artifact.name.startsWith('crawl-results-'))
            .sort((a, b) => new Date(b.created_at) - new Date(a.created_at));
          
          const toDelete = sortedArtifacts.slice(10);
          
          for (const artifact of toDelete) {
            console.log(`Deleting artifact: ${artifact.name}`);
            await github.rest.actions.deleteArtifact({
              owner: context.repo.owner,
              repo: context.repo.repo,
              artifact_id: artifact.id
            });
          }
